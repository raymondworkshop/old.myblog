<h4 id="clustering---divide-a-set-of-objects-into-meaningful-groups">Clustering - divide a set of objects into meaningful groups</h4>

<h5 id="centroid-based-partitioning">Centroid-based partitioning</h5>
<ul>
  <li>
    <p>Objects in the same cluster should be similar to each other, while in different clusters should be dissimilar.</p>
  </li>
  <li>
    <p>k-center: find the k center set with the <strong>smallest radius r*</strong></p>

    <ul>
      <li>NP-hard</li>
      <li>an optimal k-circle: a 2-approximate k circle cover <a href="http://www.cse.cuhk.edu.hk/~taoyf/course/cmsc5724/spr15/cmsc5724.html" title="Data Mining and Knowledge Discovery">1</a>
        <ul>
          <li>
            <p>returning a k-center set with radius at most 2r*</p>
          </li>
          <li>
            <p>choose <strong>a random point</strong> firstly, then choose the MAX distance to the points</p>
          </li>
        </ul>
      </li>
      <li>k-mean
        <ul>
          <li><strong>k random points</strong> as the initial centroid, form k clusters by assigning all points to the closest centroid
            <ul>
              <li>the centroid is the <strong>average of all the coordinates of the points in this cluster</strong></li>
              <li>terminate until the the centorid set don’t update.
  +</li>
            </ul>
          </li>
          <li>
            <p>k-means alg always terminates</p>

            <ul>
              <li>only a finite number of centroid sets that can possibily be produced at the end of each round</li>
              <li>after each round, the cost (the distance) of the centroid set is strictly lower than that of the old centroid set</li>
            </ul>
          </li>
          <li>the accuracy guarantee <a href="http://www.cse.cuhk.edu.hk/~taoyf/course/cmsc5724/spr15/cmsc5724.html" title="Data Mining and Knowledge Discovery">1</a>
            <ul>
              <li>
                <p>k-seeding : the seed choice (David Arthur, Sergei Vassilvitskii: k-means++: the advantages of careful seeding. SODA
2007: 1027-1035.)
each point is chosen as the centroid <strong>with a probability proportional to ( D(p)^2 )</strong>.</p>
              </li>
              <li>
                <p>if 100%, that’s k-center</p>
              </li>
              <li>
                <p>this gives the fact that the initial centroid set is picked too arbitrarily.
By doing so more carefully, we can significantly improve the approximation ratio.</p>
              </li>
            </ul>
          </li>
          <li>the limitation of k-mean
            <ul>
              <li>differing sizes, differing density, Non-globular shapes</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="hierarchical-methods">Hierarchical Methods</h5>
<ul>
  <li>Why
    <ul>
      <li>when a clustering needs, different users can explore the hierarchy to obtain <strong>various</strong> clustering results efficiently</li>
    </ul>
  </li>
  <li>How: the agglomerative method
    <ul>
      <li>merge the most similar two clusters until only one cluster is left</li>
    </ul>
  </li>
  <li>Given a dendrogram (the merging history can be represented as a tree), we can obtain k clusters
    <ul>
      <li>the alg:
        <ul>
          <li><strong>binary search tree (BST)</strong> T is used to store the distances of all pairs of the current clusters</li>
          <li>each time, remove the smallest cluster-pair distance from T, and merge them into a new cluster</li>
          <li>
            <p>O(n^2 * log n)</p>
          </li>
          <li>distance function is the key
            <ul>
              <li>distance graph G(V,E) (TODO)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="density-based">Density-based</h5>
<ul>
  <li>TODO</li>
</ul>

