I"7<blockquote>
  <p><small>[updating] An overview of Natural Language Processing and Linguistics </small></p>
</blockquote>

<h4 id="human-language">Human language</h4>
<p>A human language is a symbolic signaling system . Most words are just symbols for an extra-linguistic entity: the word is a signifier that maps to a signified (idea or thing).</p>

<p>The symbols of language can be encoded like voice, gesture, writing, etc via continuous signals to the brain. Thus exploring the continuous encoding signals can be psychology or cognitive problems (like “What is thought?”, even the fundational challenge: Turing Test ).</p>

<p><del>The core questions in human language are that problems, architectures, cognitive science, and the details of human language, how it is learned, processed, and how it changes .</del></p>

<h4 id="about-languages">About Languages</h4>
<ul>
  <li>phonetics</li>
  <li>phonology - the study of the <strong>sound patterns</strong> of human languages
    <ul>
      <li>word stress - vowels in unstressed syllables are pronounced as <strong>schwa /ə/</strong></li>
      <li>To produce a stressed syllable, one may <strong>change the pitch, make the syllable louder, or make it longer</strong></li>
      <li>Intonation may reflect syntactic or semantic differences</li>
    </ul>
  </li>
  <li>morphology - rules of word formation
    <ul>
      <li>Morphemes - the mini units of meaning</li>
    </ul>
  </li>
  <li>syntax</li>
  <li>semantics - the linguistic meaning
    <ul>
      <li>lexical semantics</li>
    </ul>
  </li>
  <li>pragmatics - how context affects meanings</li>
</ul>

<h4 id="word-vectors--">Word vectors -</h4>
<ul>
  <li>produce dense vector representations based on the context of words</li>
  <li>Count-based like TF-IDF</li>
  <li>Distributed Representations</li>
</ul>

<h4 id="word-embeddings">Word embeddings</h4>
<ul>
  <li>Word semantic meaning
    <ul>
      <li>hypernyms (is-a) relationships and synonym sets (like wordnet)</li>
      <li>word vectors encode valuable semantic information. For example, <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec Model</a>  knows a word from its neighbors, or vice versa, which relies on the linguistic hypothesis, distributional similarity (similar words have similar context).</li>
    </ul>
  </li>
</ul>

<h4 id="language-modeling---a-probabilistic-model-of-word-sequences">Language Modeling - a probabilistic model of word sequences</h4>
<ul>
  <li>N-gram</li>
  <li>Finite automata and RNN</li>
  <li>LSTM Networks</li>
</ul>

<h4 id="parsing-and-tree-structure">Parsing and tree structure</h4>

<h4 id="todo">#TODO</h4>

<h4 id="reference">reference</h4>
<ul>
  <li><a href="http://prompt.nou.edu.tw/web/sno204/content/w01/01-02-01t.htm">自動機理論</a></li>
  <li><a href="http://www.cs.umd.edu/class/fall2017/cmsc723/">Computational Linguistics I</a></li>
  <li><a href="http://www.cs.umd.edu/class/fall2016/cmsc723//lectures/">Computational Linguistics - fall2016</a></li>
  <li><a href="https://github.com/oxford-cs-deepnlp-2017/lectures">https://github.com/oxford-cs-deepnlp-2017/lectures</a></li>
  <li><a href="http://web.stanford.edu/class/cs224n/index.html">CS224n: Natural Language Processing with Deep Learning</a></li>
  <li><a href="http://ruder.io/deep-learning-nlp-best-practices/index.html#introduction">NLP best practices</a></li>
  <li><a href="http://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning">A look at the importance of Natural Language Processing</a></li>
  <li>Manning (2016). Computational Linguistics and Deep Learning. Computational Linguistics</li>
  <li>Language Files: Materials for an Introduction to Language and Linguistics</li>
  <li><a href="https://github.com/oxford-cs-deepnlp-2017/lectures">Deep Natural Language Processing</a></li>
  <li><a href="http://pquentin.github.io/nnlp/nnlp.html">A Primer on Neural Network Models for Natural Language Processing</a></li>
  <li><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec Tutorial - The Skip-Gram Model</a></li>
</ul>
:ET